
@prefix : <http://example.org/> . 
@prefix dct: <http://purl.org/dc/terms/> . 
@prefix void: <http://rdfs.org/ns/void#> . 
@prefix dcat: <http://www.w3.org/ns/dcat#> . 
@prefix foaf: <http://xmlns.com/foaf/0.1/> . 
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix schema: <http://schema.org/> .
@prefix adms:<http://www.w3.org/TR/vocab-adms/>.
@prefix prov: <http://www.w3.org/ns/prov#> .
@prefix vcard: <http://www.w3.org/2006/vcard/ns#> .  # vCard for contact details

:wikilinks-rdf-nif a dcat:Dataset ;
    dcat:keyword "format-nif", "linguistic", "linguistics", "lod", "ner", "nif", "rdf" ;
    dct:license <http://www.opendefinition.org/licenses/cc-by> ;
    dct:title_main "Wikilinks_RDF/NIF" ;
    void:exampleResource [
        dct:title_exampleResource "Example_resource" ;
        dcat:mediaType_exampleResource "text/html;_charset=UTF-8" ;
        dct:description_exampleResource """
Example converted webpage with multiple paragraphs containing wiki links
""" ;
        dcat:accessURL_exampleResource <http://wiki-link.nlp2rdf.org/linkeddata.php?t=url&f=html&i=http%3A%2F%2Fcommons.wikimedia.org%2Fwiki%2FFile%3ACana_Island_Light_-_Door_County_Wisconsin.jpg#char%3D0%2C520> ;
        adms:status_exampleResource """
OK
""" ;
    ] ;
    dcat:distribution [
        dcat:mediaType_distribution "text/html;charset=UTF-8" ;
        dct:description_distribution """
Contains the whole corpus as 110 gzipped turtle dump files.
""" ;
        adms:status_distribution """
FAIL (('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))
""" ;
        dct:title_distribution "Dump_download_directory" ;
        dct:mirror_distribution "[]" ;
        dcat:accessURL_distribution <http://wiki-link.nlp2rdf.org/dumps/> ;
    ] ;
    dcat:distribution [
        dcat:mediaType_distribution "application/x-gzip" ;
        dct:description_distribution """
Gzipped CSV file containing the data core:

"Link text","DBpedia URL","Linked Data URL","NE Type"

 Where Linked Data URL contains a link to the respective resource on this server and NE Type means the type of the resource (like Person, Location, etc). Note that the type is missing in many of the cases, due to the sheer size of the dataset and heterogenity of the data itself. 
""" ;
        adms:status_distribution """
FAIL (('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))
""" ;
        dct:title_distribution "Core_data_as_CSV" ;
        dct:mirror_distribution "[]" ;
        dcat:accessURL_distribution <http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz> ;
    ] ;
    dcat:distribution [
        dcat:mediaType_distribution "" ;
        dct:description_distribution """
DataID describing the dataset and providing some metadata like number of triples, links, etc as well as links to all distribution files.
""" ;
        adms:status_distribution """
FAIL (('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))
""" ;
        dct:title_distribution "LOD_Wikilinks_corpus_DataID_file" ;
        dct:mirror_distribution "[]" ;
        dcat:accessURL_distribution <http://wiki-link.nlp2rdf.org/dataid.ttl> ;
    ] ;
    dct:description_main """
The [Wikilinks corpus](http://www.iesl.cs.umass.edu/data/wiki-links) is a coreference resolution corpus of very large scale. It contains over 40 million mentions of over 3 million entities. Mentions are manually labeled links to respective Wikipedia pages in natural language text. They are obtained via a web crawl and aggregated together with the pages' source in the extended corpus of over 180GB in size.

We took the corpus and converted it into the [NLP Interchange Format (NIF)](http://nlp2rdf.org/), publishing it here in Linked Open Data, RDF dumps and an accompanying CSV. 

Every webpage in the corpus was parsed. The text of the html element surrounding the individual Wikipedia links was extracted and concatenated together, if there was more than one link on the page. The position of the links in these texts was located and annotated via string offsets. The position of the html elements containing the links was annotated with Xpath expressions. For every link to Wikipedia, the respective [DBpedia](http://dbpedia.org) page was included as a link. The [DBpedia ontology classes](http://wiki.dbpedia.org/Ontology) of the linked resource were added as well. If a mapping exists, [NERD core classes](http://nerd.eurecom.fr/) were added, too.

The data is available in the Apache file system under [http://wiki-link.nlp2rdf.org/data/](http://wiki-link.nlp2rdf.org/data/). However, for ease of use, it is also available in a number of [gzipped Dumpfiles](http://wiki-link.nlp2rdf.org/dumps/). Additionally, there is a [gzipped CSV file](http://wiki-link.nlp2rdf.org/wikilink_ne.csv.tar.gz) containing the core of the data. 
"""@en ;

    void:triples "533016300"^^xsd:integer ;
    dcat:theme "linguistics" ;
    prov:qualifiedAttribution [
        prov:Cagent :contactAgent ;
        dcat:hadRole :contact_point ;
    ] .

@prefix xsd: <http://www.w3.org/2001/XMLSchema#> . 
:dbpedia a void:Linkset ;
    void:target :wikilinks-rdf-nif ;
    void:triples "31542468"^^xsd:integer ;
 .


:contactAgent a prov:Agent ;
     foaf:Cname "Martin_Br√ºmmer"^^xsd:string ;
     foaf:Cmbox <mailto:bruemmer@inform@ik.uni-leipzig.de> ; 
 .

